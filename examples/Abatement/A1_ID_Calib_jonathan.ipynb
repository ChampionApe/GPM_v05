{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file_gams_py_gdb1.gdx is still active and was not deleted.\n"
     ]
    }
   ],
   "source": [
    "clean_up=True # removes gams-related files in work-folder if true\n",
    "%run StdPackages.ipynb\n",
    "os.chdir(directory['py'])\n",
    "import techdata_to_tree, sys, ShockFunction\n",
    "import abatement_ID_calib as abatement\n",
    "os.chdir(directory['curr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Set up tree and read in data, akin to A1\\_ID\\_Calib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Read data:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfile = \"techdata_dors_2.xlsx\" # file\n",
    "inputoth  = 'Othdata_dors_2.xlsx'\n",
    "output = techdata_to_tree.load_techcats(pd.read_excel(directory['data'] + \"/\" + inputfile, sheet_name=[\"inputdisp\", \"endofpipe\", \"inputprices\"]))\n",
    "modules = [\"ID\", \"EOP\"] # modules to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Set up trees:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nts = {m: nesting_tree.nesting_tree(name=m) for m in modules}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1: Input displacing tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nts[\"ID\"].add_tree(output[\"ID\"][\"upper_categories\"], tree_name = 'ID_EC', **{\"type_f\":\"CES_norm\"}) # E to C\n",
    "nts[\"ID\"].add_tree(output[\"ID\"][\"components\"], tree_name = \"ID_CU\", **{\"type_f\":\"MNL\"}) # C to U\n",
    "nts[\"ID\"].add_tree(output[\"ID\"][\"techs\"], tree_name=\"ID_TU\", **{'type_io': 'output', 'type_f': 'CET_norm'}) # U to T\n",
    "nts[\"ID\"].add_tree(output[\"ID\"][\"techs_inputs\"], tree_name=\"ID_TX\") # T to inputs X\n",
    "nts[\"ID\"].add_tree(output[\"ID\"][\"basetechs\"], tree_name=\"ID_BU\", **{\"type_io\":\"output\", \"type_f\":\"linear_out\"}) # baseline technologies\n",
    "nts[\"ID\"].add_tree(output[\"ID\"][\"basetech_inputs\"], tree_name=\"ID_BX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2: End of pipe tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nts[\"EOP\"].add_tree(output[\"EOP\"][\"components\"], tree_name = \"EOP_CU\", **{\"type_f\":\"MNL\"}) # C to U\n",
    "nts[\"EOP\"].add_tree(output[\"EOP\"][\"techs\"], tree_name=\"EOP_TU\", **{'type_io': 'output', 'type_f': 'CET_norm'}) # T to U \n",
    "nts[\"EOP\"].add_tree(output[\"EOP\"][\"techs_inputs\"], tree_name=\"EOP_TX\") # T to inputs X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3: Trees related to final goods and emissions account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Read in tree and data from xlsx:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nts['ID'].add_tree(directory['data']+'\\\\'+inputoth,tree_name='ID_Y',**{'sheet':'Y'})\n",
    "DataBase.GPM_database.merge_dbs(nts['ID'].trees['ID_Y'].database,excel2py.xl2PM.pm_from_workbook(directory['data']+'\\\\'+inputoth,{'Y': 'vars'}), 'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Add new Q2P:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['ID']['Q2P'] = output['ID']['Q2P'].append(excel2py.xl2PM.pm_from_workbook(directory['data']+'\\\\'+inputoth,{'Q2P': 'maps'}).database['Q2P'].vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3: Namespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Give modules local namespaces:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_sets = ('inp','out','int','wT','map_all','kno_out','kno_inp')\n",
    "for module in modules:\n",
    "    namespace = {k: module + '_' + k for k in standard_sets}\n",
    "    nts[module].run_all(**namespace)\n",
    "    #Also replaces keys with module-specific names, i.e. changes 'inp' to 'ID_inp' in the attributes/keys.\n",
    "    for std_set in standard_sets:\n",
    "        if hasattr(nts[module], std_set):\n",
    "            setattr(nts[module], module + \"_\" + std_set, getattr(nts[module], std_set))\n",
    "            delattr(nts[module], std_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Add emissions data:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataBase.GPM_database.merge_dbs(nts['ID'].database,excel2py.xl2PM.pm_from_workbook(directory['data']+'\\\\'+inputoth, {'M': 'vars','M_sets': 'sets'}),'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Initialize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = abatement.abate(nt=nts['ID'],tech=output,work_folder=directory['work'],**{'data_folder':directory['gams'],'name': 'atest'})\n",
    "gm.model.settings.tech_catalog = \"techdata_dors_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Add a couple of extra aliases, and update sets with elements:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.add_aliases([('n','nnnn'),('n','nnnnn')])\n",
    "gm.model.database.update_all_sets(clean_up=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.initialize_variables_leontief()\n",
    "gm.initialize_variables(**{'check_variables': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_star = DataBase.GPM_database(**{'name': 'db_star'})\n",
    "sigma_star = gm.get(\"sigma\").copy()\n",
    "eta_star = gm.get(\"eta\").copy()\n",
    "mu_star = gm.get(\"mu\").copy()\n",
    "\n",
    "eta_star[:] = -3\n",
    "sigma_star[gm.get(\"kno_ID_CU\")] = 0.2\n",
    "sigma_star[gm.get(\"kno_no_ID_BX\")] = 0.5\n",
    "sigma_star[\"Y\"] = 0.2\n",
    "\n",
    "mu_star[gm.get(\"map_ID_CU\")] = 1.0\n",
    "\n",
    "db_star[\"sigma\"] = sigma_star\n",
    "db_star[\"eta\"] = eta_star\n",
    "# db_star[\"mu\"] = mu_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='v0'\n",
    "gm.write_and_run(name=name, add_checkpoint=name)\n",
    "gm.checkpoints['v1'] = gm.model_instances[name].ws.add_checkpoint()\n",
    "gm.model_instances[name].solve_sneakily(db_star=db_star,from_cp = True, cp_init = gm.checkpoints[name], options_run={'checkpoint': gm.checkpoints['v1']})\n",
    "gm.model.database.merge_dbs(gm.model.database,gm.model_instances[name].out_db,'second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_to_shock = \"C_EL_1\"\n",
    "db_star = DataBase.GPM_database(**{'name': 'db_star'})\n",
    "x_var = \"sigma\"\n",
    "sigma_star = gm.get(x_var).copy()\n",
    "sigma_star[node_to_shock] = 0.8\n",
    "db_star[x_var] = sigma_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "(shock_db, shock_kwargs) = ShockFunction.sneaky_db(gm.model.database, db_star, n_steps = 160, loop_name = x_var + '_loop')\n",
    "store_sol = {\"PwThat\":{'domains': shock_kwargs['loop_name']}, \"qD\":{'domains': shock_kwargs['loop_name']}, \"sigma\":{'domains': shock_kwargs['loop_name']}}\n",
    "mi = gm.model_instances[name]\n",
    "shock = mi.std_UEVAS_from_db(shock_db, loop_name=shock_kwargs['loop_name'], shock_name=shock_kwargs['shock_name'], store_sol=store_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi.execute_shock_from_cp(shock=shock, cp=gm.checkpoints['v1'])\n",
    "gm.model.database.merge_dbs(gm.model.database,mi.out_db,'second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nodes_to_save = {\n",
    "    \"techdata_dors_2\":[\"ID_t2\", \"U_ID_t2_1\", \"C_EL_1\", \"U0_ID_C_EL_1\"]\n",
    "}\n",
    "sols = {key:mi.out_db.series[\"sol_\" + key].vals for key in store_sol.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_sol_data(sols, x_var, nodes_to_save, node_to_shock, tech_catalog):\n",
    "    list_of_dfs = []\n",
    "    for var in sols:\n",
    "        if var == x_var:\n",
    "            continue\n",
    "        df = sols[var]\n",
    "        if len(df.index.get_level_values(\"n\").unique()) == 1:\n",
    "            only_one_series = True\n",
    "        else:\n",
    "            only_one_series = False\n",
    "        df = df.reset_index()\n",
    "        df2 = df.pivot_table(df.columns[-1], df.columns[0], df.columns[1]).reset_index()\n",
    "        if gm.model.settings.tech_catalog in nodes_to_save and not only_one_series:\n",
    "            df2 = df2[[x_var + \"_loop\"] + [c for c in nodes_to_save[tech_catalog] if c in df2.columns]]\n",
    "        df3 = df2.merge(sols[x_var].iloc[sols[x_var].index.get_level_values(1) == node_to_shock].reset_index().drop(columns=\"n\"), \\\n",
    "                        how=\"inner\", on=x_var+\"_loop\").drop(columns=x_var+\"_loop\")\n",
    "        df3 = df3.rename(columns={\"sol_\"+x_var:x_var}).set_index(x_var).sort_values(by=x_var)\n",
    "        if only_one_series:\n",
    "            df3.rename(columns={df3.columns[0]:var}, inplace=True)\n",
    "        else:\n",
    "            df3.rename(columns={key:var + \"_\"+ key for key in df3.columns}, inplace=True)\n",
    "        list_of_dfs.append(df3)\n",
    "    return list_of_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = {}\n",
    "out[name] = organize_sol_data(sols=sols, x_var=\"sigma\", nodes_to_save=nodes_to_save, node_to_shock=node_to_shock, tech_catalog=gm.model.settings.tech_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\zgr679\\\\Documents\\\\GitHub\\\\GPM_v05\\\\examples\\\\Abatement\\\\gamsmodels\\\\medium_sigma'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[gm.model.database.series.__delitem__(sym) for sym in [\"sol_\" + k for k in sols.keys()]]; # delete the symbols created from the loop\n",
    "[gm.model.database.series.__delitem__(sym) for sym in gm.model.database.symbols if \"loop\" in sym]; # delete the symbols created from the loop\n",
    "gm.export(name=\"medium_sigma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second round, closer to where the error happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_star[\"sigma\"].vals[node_to_shock] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"v2\"\n",
    "(shock_db, shock_kwargs) = ShockFunction.sneaky_db(gm.model.database, db_star, n_steps = 240, loop_name=x_var+'_loop')\n",
    "store_sol = {\"PwThat\":{'domains': shock_kwargs['loop_name']}, \"qD\":{'domains': shock_kwargs['loop_name']}, \"sigma\":{'domains': shock_kwargs['loop_name']}}\n",
    "gm.write_and_run(name=name, add_checkpoint=name)\n",
    "mi = gm.model_instances[name]\n",
    "shock = mi.std_UEVAS_from_db(shock_db, loop_name=shock_kwargs['loop_name'], shock_name=shock_kwargs['shock_name'], store_sol=store_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi.execute_shock_from_cp(shock=shock, cp=gm.checkpoints[name])\n",
    "sols = {key:mi.out_db.series[\"sol_\" + key].vals for key in store_sol.keys()}\n",
    "gm.model.database.merge_dbs(gm.model.database,mi.out_db,'second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[name] = organize_sol_data(sols=sols, x_var=\"sigma\", nodes_to_save=nodes_to_save, node_to_shock=node_to_shock, tech_catalog=gm.model.settings.tech_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export as pickle\n",
    "df_out = pd.concat([pd.concat(out[\"v0\"], axis = 1), pd.concat(out[\"v2\"], axis = 1)], axis=0)\n",
    "df_out.to_pickle(x_var + \"_\" + gm.model.settings.tech_catalog + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\zgr679\\\\Documents\\\\GitHub\\\\GPM_v05\\\\examples\\\\Abatement\\\\gamsmodels\\\\high_sigma'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Export the model\n",
    "[gm.model.database.series.__delitem__(sym) for sym in [\"sol_\" + k for k in sols.keys()]]; # delete the symbols created from the loop\n",
    "[gm.model.database.series.__delitem__(sym) for sym in gm.model.database.symbols if \"loop\" in sym]; # delete the symbols created from the loop\n",
    "gm.export(name=\"high_sigma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
